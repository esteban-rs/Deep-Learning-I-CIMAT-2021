{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1. Algoritmos de descenso estocástico\n",
    "## Aprendizaje de Máquina I\n",
    " Esteban Reyes Saldaña"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar versiones estocásticas de algoritmos de descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descenso de gradiente estocśtico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD (theta = [], grad = None, gd_params = [], f_params = []):\n",
    "    '''\n",
    "    Descenso Gradiente Estocástico\n",
    "    \n",
    "    PARÁMETROS\n",
    "    ------------\n",
    "    theta      : condición inicial\n",
    "    grad       : función que calcula el gradiente\n",
    "    \n",
    "    gd_params  : lista de parámetros para el algoritmo de descenso,\n",
    "                    nIter = gd_params['nIter'] número de iteraciones\n",
    "                    alpha = gd_params['alpha'] tamaño de paso alpha\n",
    "                    batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "    \n",
    "    f_params   : lista de parámetros para la función objetivo\n",
    "                    kappa = f_params['kappa'] parámetro de escala (rechazo de outlies)\n",
    "                    X     = f_params['X']     variable independiente\n",
    "                    y     = f_params['y']     variable dependiente\n",
    "    \n",
    "    Regresa\n",
    "    ------------\n",
    "    Theta      : trayectoria de parametros\n",
    "                    Theta[-1] es el valor alcanzado en la última iteración\n",
    "    '''\n",
    "    \n",
    "    (high, dim) = f_params['X'].shape\n",
    "    batch_size  = gd_params['batch_size']\n",
    "    \n",
    "    nIter       = gd_params['nIter']\n",
    "    alpha       = gd_params['alpha']\n",
    "    \n",
    "    Theta = []\n",
    "    \n",
    "    for t in range(nIter):\n",
    "        # Set of sampled indices\n",
    "        smpIdx = np.random.randint(low = 0, high = high, size = batch_size, dtype = 'int32')\n",
    "        #sample\n",
    "        smpX = f_params['X'][smpIdx]\n",
    "        smpy = f_params['y'][smpIdx]\n",
    "        \n",
    "        # parámetros de la función objetivo\n",
    "        smpf_params = { **f_params, \n",
    "                       'kappa' : f_params['kappa'], \n",
    "                        'X'    : smpX , \n",
    "                        'y'    : smpy\n",
    "                       }\n",
    "        \n",
    "        p = grad(theta, f_params = smpf_params)\n",
    "        theta = theta - alpha*p\n",
    "        Theta.append(theta)\n",
    "        \n",
    "    return np.array(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Descenso de gradiente estocástico accelerado de tipo Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAG(theta = [], grad = None, gd_params = {}, f_params = {}) :\n",
    "    '''\n",
    "    Descenso Acelerado de Nesterov\n",
    "    \n",
    "    Parámetros\n",
    "    -----------\n",
    "    theta     : condición incial\n",
    "    grad      : función que calcula el gradiente\n",
    "    gd_params : lista de parámetros para el algoritmo de descenso,\n",
    "                    nIter = gd_params['nIter'] número de iteraciones\n",
    "                    alpha = gd_params['alpha'] tamaño de paso alpha\n",
    "                    eta   = gd_params['eta']   parámetro de inercia (0,1]\n",
    "                    \n",
    "                    batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "                    \n",
    "    f_params   : lista de parámetros para la función objetivo,\n",
    "                    kappa = f_params['kappa'] parámetro de escala (rechazo de outliers)\n",
    "                    X     = f_params['X']     variable independiente\n",
    "                    y     = f_params['y']     variable dependiente\n",
    "    \n",
    "    Regresa\n",
    "    ----------\n",
    "    Theta     : trayectoria de los parámetros\n",
    "                    Theta[-1] es el valor alcanzado en la última iteración\n",
    "    '''\n",
    "    (high, dim) = f_params['X'].shape\n",
    "    batch_size  = gd_params['batch_size']\n",
    "    \n",
    "    nIter = gd_params['nIter']\n",
    "    alpha = gd_params['alpha']\n",
    "    eta   = gd_params['eta']\n",
    "    \n",
    "    p     = np.zeros(theta.shape)\n",
    "    \n",
    "    Theta = []\n",
    "    \n",
    "    for t in range(nIter):\n",
    "        # Set of sampled indices\n",
    "        smpIdx = np.random.randint(low = 0, high = high, size = batch_size, dtype = 'int32')\n",
    "        #sample\n",
    "        smpX = f_params['X'][smpIdx]\n",
    "        smpy = f_params['y'][smpIdx]\n",
    "        \n",
    "        # parámetros de la función objetivo\n",
    "        smpf_params = { **f_params, \n",
    "                       'kappa' : f_params['kappa'], \n",
    "                        'X'    : smpX , \n",
    "                        'y'    : smpy\n",
    "                       }\n",
    "        \n",
    "        \n",
    "        pre_theta = theta - 2.0*alpha*p\n",
    "        g = grad(pre_theta, f_params = smpf_params)\n",
    "        p = g + eta*p\n",
    "        theta = theta - alpha*p\n",
    "        Theta.append(theta)\n",
    "        \n",
    "    return np.array(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADADELTA(theta = [], grad = None, gd_params = {}, f_params = {}):\n",
    "    '''\n",
    "    Descenso de Gradiente Adaptable (ADADELTA)\n",
    "    \n",
    "    Parámetros\n",
    "    -----------\n",
    "    theta     : Condición Inicial\n",
    "    grad      : función que calcula el gradiente\n",
    "    gd_params : lista de parámetros para el algoritmo de descenso,\n",
    "                    nIter    = gd_params['nIter'] número de iteraciones\n",
    "                    alphaADA = gd_params['alphaDELTA'] tamaño de paso alpha\n",
    "                    eta      = gd_params['eta'] parámetro adaptación del alpha\n",
    "    \n",
    "                    batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "\n",
    "                    \n",
    "    f_params  : lista de parámetros para la función objetivo,\n",
    "                    kappa = f_params['kappa'] parámetro de escala (recha<o de outliers)\n",
    "                    X     = f_params['X'] variable independiente\n",
    "                    y     = f_params['y'] variable dependiente\n",
    "    \n",
    "    Regresa\n",
    "    -----------\n",
    "    Theta     : trayectoria de los parámetros\n",
    "                    Theta[-1] es el valor alcanzado en la última iteración\n",
    "    \n",
    "    '''\n",
    "    (high, dim) = f_params['X'].shape\n",
    "    batch_size  = gd_params['batch_size']\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    nIter   = gd_params['nIter']\n",
    "    alpha   = gd_params['alphaADADELTA']\n",
    "    eta     = gd_params['eta']\n",
    "    G       = np.zeros(theta.shape)\n",
    "    g       = np.zeros(theta.shape)\n",
    "    \n",
    "    Theta = []\n",
    "    \n",
    "    for t in range(nIter):\n",
    "        # Set of sampled indices\n",
    "        smpIdx = np.random.randint(low = 0, high = high, size = batch_size, dtype = 'int32')\n",
    "        #sample\n",
    "        smpX = f_params['X'][smpIdx]\n",
    "        smpy = f_params['y'][smpIdx]\n",
    "        \n",
    "        # parámetros de la función objetivo\n",
    "        smpf_params = { **f_params, \n",
    "                       'kappa' : f_params['kappa'], \n",
    "                        'X'    : smpX , \n",
    "                        'y'    : smpy\n",
    "                       }\n",
    "        \n",
    "        g = grad(theta, f_params=smpf_params)\n",
    "        G = eta*g**2 + (1-eta)*G\n",
    "        p = 1.0/(np.sqrt(G)+epsilon)*g\n",
    "        theta = theta - alpha * p\n",
    "        Theta.append(theta)\n",
    "    \n",
    "    return np.array(Theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM(theta=[], grad=None, gd_params={}, f_params={}):\n",
    "    '''\n",
    "    Descenso de Gradiente Adaptable con Momentum (ADAM) \n",
    "    \n",
    "    Parámetros\n",
    "    -----------\n",
    "    theta     :   condicion inicial\n",
    "    grad      :   funcion que calcula el gradiente\n",
    "    gd_params :   lista de parametros para el algoritmo de descenso, \n",
    "                      nIter    = gd_params['nIter'] número de iteraciones\n",
    "                      alphaADA = gd_params['alphaADAM'] tamaño de paso alpha\n",
    "                      eta1     = gd_params['eta1'] factor de momentum para la direccion \n",
    "                                 de descenso (0,1)\n",
    "                      eta2     = gd_params['eta2'] factor de momentum para la el \n",
    "                                 tamaño de paso (0,1)\n",
    "                                 \n",
    "                      batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "                  \n",
    "    f_params  :   lista de parametros para la funcion objetivo, \n",
    "                      kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
    "                      X     = f_params['X'] Variable independiente\n",
    "                      y     = f_params['y'] Variable dependiente                   \n",
    "\n",
    "    Regresa\n",
    "    -----------\n",
    "    Theta     :   trayectoria de los parametros\n",
    "                     Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "    '''\n",
    "    (high, dim) = f_params['X'].shape\n",
    "    batch_size  = gd_params['batch_size']\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    nIter   = gd_params['nIter']\n",
    "    alpha   = gd_params['alphaADAM'] \n",
    "    eta1    = gd_params['eta1']\n",
    "    eta2    = gd_params['eta2']\n",
    "    p       = np.zeros(theta.shape)\n",
    "    v       = 0.0\n",
    "    \n",
    "    Theta   = []\n",
    "    eta1_t  = eta1\n",
    "    eta2_t  = eta2\n",
    "\n",
    "    for t in range(nIter):\n",
    "        # Set of sampled indices\n",
    "        smpIdx = np.random.randint(low = 0, high = high, size = batch_size, dtype = 'int32')\n",
    "        #sample\n",
    "        smpX = f_params['X'][smpIdx]\n",
    "        smpy = f_params['y'][smpIdx]\n",
    "        \n",
    "        # parámetros de la función objetivo\n",
    "        smpf_params = { **f_params, \n",
    "                       'kappa' : f_params['kappa'], \n",
    "                        'X'    : smpX , \n",
    "                        'y'    : smpy\n",
    "                       }\n",
    "\n",
    "        g  = grad(theta, f_params = smpf_params)\n",
    "        \n",
    "        p  = eta1*p + (1.0 - eta1)*g\n",
    "        v  = eta2*v + (1.0 - eta2)*(g**2)\n",
    "\n",
    "        pp = p/(1.-eta1_t) \n",
    "        vv = v/(1.-eta2_t) \n",
    "        \n",
    "        theta = theta - alpha * pp / (np.sqrt(vv) + epsilon)\n",
    "        Theta.append(theta)\n",
    "        \n",
    "        eta1_t *= eta1\n",
    "        eta2_t *= eta2\n",
    "\n",
    "    return np.array(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. NADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NADAM(theta=[], grad=None, gd_params={}, f_params={}):\n",
    "    '''\n",
    "    Descenso de Gradiente Adaptable con Momentum (ADAM) \n",
    "    \n",
    "    Parámetros\n",
    "    -----------\n",
    "    theta     :   condicion inicial\n",
    "    grad      :   funcion que calcula el gradiente\n",
    "    gd_params :   lista de parametros para el algoritmo de descenso, \n",
    "                      nIter    = gd_params['nIter'] número de iteraciones\n",
    "                      alphaADA = gd_params['alphaADAM'] tamaño de paso alpha\n",
    "                      eta1     = gd_params['eta1'] factor de momentum para la direccion \n",
    "                                 de descenso (0,1)\n",
    "                      eta2     = gd_params['eta2'] factor de momentum para la el \n",
    "                                 tamaño de paso (0,1)\n",
    "                                 \n",
    "                      batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "                  \n",
    "    f_params  :   lista de parametros para la funcion objetivo, \n",
    "                      kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
    "                      X     = f_params['X'] Variable independiente\n",
    "                      y     = f_params['y'] Variable dependiente                   \n",
    "\n",
    "    Regresa\n",
    "    -----------\n",
    "    Theta     :   trayectoria de los parametros\n",
    "                     Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "    '''\n",
    "    (high, dim) = f_params['X'].shape\n",
    "    batch_size  = gd_params['batch_size']\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    nIter   = gd_params['nIter']\n",
    "    alpha   = gd_params['alphaADAM'] \n",
    "    eta1    = gd_params['eta1']\n",
    "    eta2    = gd_params['eta2']\n",
    "    p       = np.zeros(theta.shape)\n",
    "    v       = 0.0\n",
    "    \n",
    "    Theta   = []\n",
    "    eta1_t  = eta1\n",
    "    eta2_t  = eta2\n",
    "\n",
    "    for t in range(nIter):\n",
    "        # Set of sampled indices\n",
    "        smpIdx = np.random.randint(low = 0, high = high, size = batch_size, dtype = 'int32')\n",
    "        #sample\n",
    "        smpX = f_params['X'][smpIdx]\n",
    "        smpy = f_params['y'][smpIdx]\n",
    "        \n",
    "        # parámetros de la función objetivo\n",
    "        smpf_params = { **f_params, \n",
    "                       'kappa' : f_params['kappa'], \n",
    "                        'X'    : smpX , \n",
    "                        'y'    : smpy\n",
    "                       }\n",
    "        \n",
    "        \n",
    "        pre_theta = theta - 2.0*alpha*p\n",
    "        g  = grad(pre_theta, f_params = smpf_params)\n",
    "        \n",
    "        p  = eta1*p + (1.0 - eta1)*g\n",
    "        v  = eta2*v + (1.0 - eta2)*(g**2)\n",
    "\n",
    "        pp = p/(1.-eta1_t) \n",
    "        vv = v/(1.-eta2_t) \n",
    "        \n",
    "        theta = theta - alpha * pp / (np.sqrt(vv) + epsilon)\n",
    "        Theta.append(theta)\n",
    "        \n",
    "        eta1_t *= eta1\n",
    "        eta2_t *= eta2\n",
    "        \n",
    "    return np.array(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver el problema de regresión\n",
    "\n",
    "\n",
    "$$ \\min_{\\alpha, \\mu} \\dfrac{1}{2} || \\Phi \\alpha - y ||^2_2 $$\n",
    "\n",
    "donde \n",
    "\n",
    "$$ \\Phi = [\\phi_1, \\phi_2, \\dots, \\phi_n ] $$\n",
    "\n",
    "con \n",
    "\n",
    "$$ \\phi_{ij} = \\exp \\left( - \\dfrac{1}{2\\sigma^2} (j - \\mu_i)^2 \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para Actualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi (Y, mu, sigma, n) :\n",
    "    ''' \n",
    "    Construye  Matriz de Kerneles Phi\n",
    "    \n",
    "    Parámetros\n",
    "    -----------\n",
    "        Y            : Patrones a Aproximar\n",
    "        mu           : Array de medias\n",
    "        sigma        : Vector de Desviaciones\n",
    "        num_rad_func : Número de funciones radiales usadas\n",
    "    Regresa\n",
    "    -----------\n",
    "        phi          : matriz de kerneles\n",
    "    '''\n",
    "    phi = np.zeros((Y.shape[0], n))\n",
    " \n",
    "    for i in range(n):\n",
    "        phi[:, i] = np.exp(- 1.0 / (2*sigma**2) * (Y - mu[i])**2)\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_gaussian_radial_mu(theta, f_params) :\n",
    "    '''\n",
    "    Calcula el gradiente respecto a mu\n",
    "    Parámetros\n",
    "    -----------\n",
    "        theta\n",
    "        f_params : lista de parametros para la funcion objetivo, \n",
    "                      kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
    "                      X     = f_params['X'] Variable independiente\n",
    "                      y     = f_params['y'] Variable dependiente    \n",
    "\n",
    "    Regresa\n",
    "    -----------\n",
    "        Array gradiente\n",
    "    '''\n",
    "    # Obtengo Parámetros\n",
    "    phi   = f_params['X']\n",
    "    alpha = f_params['Alpha']\n",
    "    n     = f_params['n']\n",
    "    Y     = f_params['y']\n",
    "    mu    = f_params['mu']\n",
    "\n",
    "    \n",
    "    #gradient = (phi @ alpha - Y).reshape((Y.shape[0], 1)) * alpha.T * (Y.reshape((Y.shape[0], 1))* np.ones((1, n)) - np.ones((Y.shape[0], 1)) * mu.T) \n",
    "    \n",
    "    gradient = (phi @ alpha - Y) @ alpha.T  * (Y @ np.ones((Y.shape[0], n)) - np.ones((1, Y.shape[0])) @ mu)\n",
    "\n",
    "    return np.mean(gradient, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_gaussian_radial_alpha(theta, f_params) :\n",
    "    '''\n",
    "    Calcula el gradiente respecto a alpha\n",
    "    Parámetros\n",
    "    -----------\n",
    "        theta\n",
    "        f_params : lista de parametros para la funcion objetivo, \n",
    "                      kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
    "                      X     = f_params['X'] Variable independiente\n",
    "                      y     = f_params['y'] Variable dependiente    \n",
    "\n",
    "    Regresa\n",
    "    -----------\n",
    "        Array gradiente\n",
    "    '''\n",
    "    # Obtengo Parámetros\n",
    "    phi   = f_params['X']\n",
    "    Y     = f_params['y']\n",
    "    alpha = f_params['Alpha']\n",
    "    mu    = f_params['mu']\n",
    "    n     = f_params['n']\n",
    "        \n",
    "    # (phi (alpha) - Y) alpha^T \n",
    "    gradient = phi.T @ (phi @ alpha - Y) \n",
    "    # Y - mu^T\n",
    "    return np.mean(gradient, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "n        = 100\n",
    "m        = 2000\n",
    "sigma    = 1\n",
    "epsilon  = 0.01\n",
    "theta    = 10 * np.random.normal(size = 2)\n",
    "\n",
    "# parámetros del algoritmo\n",
    "gd_params = {\n",
    "                'alpha'         : 0.95, \n",
    "                'alphaADADELTA' : 0.7,\n",
    "                'alphaADAM'     : 0.95,\n",
    "                'nIter'         : 300,\n",
    "                'batch_size'    : 100,\n",
    "                'eta'           : 0.9,\n",
    "                'eta1'          : 0.9,\n",
    "                'eta2'          : 0.999\n",
    "            }\n",
    "\n",
    "# Distintos descensos de gradientes a usar\n",
    "grad_algthms = [\n",
    "                (SGD, 'SDG'),\n",
    "                (NAG, 'NAG'), \n",
    "                (ADADELTA, 'ADADELTA'), \n",
    "                (ADAM, 'ADAM'),\n",
    "                (NADAM, 'NADAM')\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def solver(algthm, Y):\n",
    "    t_init = time.clock_gettime(0) \n",
    "    \n",
    "    # Valores Iniciales\n",
    "    mu     = np.linspace(0, 100, n) \n",
    "    phi    = get_phi(Y, mu, sigma, n)\n",
    "    alpha  = np.random.uniform(0, sigma, n)\n",
    "\n",
    "    # Parámetros para el gradiente\n",
    "    f_params = {\n",
    "                    'kappa' : 0.01, \n",
    "                    'mu'    : mu, \n",
    "                    'X'     : phi,\n",
    "                    'y'     : Y, \n",
    "                    'Alpha' : alpha, \n",
    "                    'n'     : n\n",
    "                }\n",
    "    \n",
    "    num_iter = 0\n",
    "    \n",
    "    while num_iter < max_iter :\n",
    "        ''' D E S C E N S O   P A R A  A L P H A '''\n",
    "        alpha_prev = alpha\n",
    "        alpha      = algthm(alpha, grad = grad_gaussian_radial_alpha, gd_params = gd_params, f_params = f_params)[-1]\n",
    "\n",
    "        if np.linalg.norm(phi @ alpha - Y) < epsilon:\n",
    "            break\n",
    "\n",
    "        ''' D E S C E N S O  P A R A  M U '''\n",
    "        mu_old = mu\n",
    "        mu     = algthm(mu, grad = grad_gaussian_radial_mu, gd_params = gd_params, f_params = f_params)[-1]\n",
    "        \n",
    "        ''' A C T U A L I Z A C I Ó N '''\n",
    "        phi = get_phi(Y, mu, sigma, n)\n",
    "        \n",
    "        # Criterio de parada\n",
    "        if np.linalg.norm(mu - mu_old) < epsilon:\n",
    "            break\n",
    "        # Número máximo de iteraciones si no hay converrgencia\n",
    "        num_iter += 1\n",
    "        \n",
    "            \n",
    "    t_end = time.clock_gettime(0)\n",
    "    \n",
    "    total_time = t_end - t_init\n",
    "        \n",
    "    return phi, alpha, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "E J E C U C I Ó N  \n",
    "'''\n",
    "# Muestra aleatoria\n",
    "Y       = np.random.uniform(0, 1, m)\n",
    "results = [solver(algthm[0], Y) for algthm in grad_algthms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------  ----------  ----------  ----------  ----------  ----------\n",
      "Algorithm         SDG         NAG         ADADELTA    ADAM        NADAM\n",
      "Time              4.56265473  4.59919763  4.87210226  6.99568176  5.4384284\n",
      "Cummulated Error  0.32567695  0.32567695  0.32567695  0.32567695  0.32567695\n",
      "----------------  ----------  ----------  ----------  ----------  ----------\n"
     ]
    }
   ],
   "source": [
    "# Compara tiempos de ejecución\n",
    "times = [ \n",
    "            [algthm for (_, algthm) in grad_algthms], \n",
    "            [round(result[-1], 8) for result in results], \n",
    "            [round(np.square( result[0] @ result[1] - Y ).mean(), 8) for result in results]\n",
    "        ]\n",
    "\n",
    "times[0] = [\"Algorithm\"] + times[0]\n",
    "times[1] = [\"Time\"] + times[1]\n",
    "times[2] = [\"Cummulated Error\"] + times[2]\n",
    "\n",
    "print(tabulate(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
